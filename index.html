<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="description" content="We teach models to first reason about how a scene will move, before predicting what it will look like.">
  <meta property="og:title" content="FlowVLA: Thinking in Motion with a Visual Chain of Thought"/>
  <meta property="og:description" content="By forcing a model to predict intermediate motion (optical flow), we unlock more physically realistic world models and more efficient robot learning."/>
  <meta property="og:url" content="URL_OF_YOUR_PROJECT_PAGE"/> <!-- TODO: Fill this in after deploying -->
  <!-- Path to banner image -->
  <meta property="og:image" content="static/images/flowvla_architecture.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="FlowVLA: Thinking in Motion with a Visual Chain of Thought">
  <meta name="twitter:description" content="By forcing a model to predict intermediate motion (optical flow), we unlock more physically realistic world models and more efficient robot learning.">
  <!-- Path to banner image -->
  <meta name="twitter:image" content="static/images/flowvla_architecture.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper -->
  <meta name="keywords" content="World Models, Robotics, VLA, Visual Chain of Thought, Optical Flow, Physical Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>FlowVLA: Thinking in Motion with a Visual Chain of Thought</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FlowVLA: Thinking in Motion with a Visual Chain of Thought</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">Zhide Zhong,</span>
            <span class="author-block">Haodong Yan,</span>
            <span class="author-block">Junfeng Li,</span>
            <span class="author-block">Xiangchen Liu,</span>
            <span class="author-block">Xin Gong,</span>
            <span class="author-block">Wenxuan Song,</span>
            <span class="author-block">Jiayi Chen,</span>
            <span class="author-block">Haoang Li</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">HKUST(GZ)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Github link (Coming Soon) -->
              <span class="link-block">
                <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code (Coming Soon) </span>
                </a>
              </span>
              <!-- Paper PDF link -->
              <!-- <span class="link-block">
                <a href="URL_TO_YOUR_PAPER_PDF" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image -->
<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Your image here. We recommend the architecture diagram or a key result figure. -->
        <img src="static/images/flowvla_teaser.png" alt="FlowVLA Core Concept" class="teaser-image"/>
        <h2 class="subtitle has-text-centered">
          We teach models to first reason about <b>how</b> a scene will move (motion), before predicting <b>what</b> it will look like (appearance). This simple principle unlocks more physically realistic world models and more efficient robot learning.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->
  

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Many Vision-Language-Action (VLA) models rely on an internal world model trained via next-frame prediction. This approach, however, struggles with physical reasoning as it entangles static appearance with dynamic motion, often resulting in implausible visual forecasts and inefficient policy learning. To address these limitations, we introduce the <strong>Visual Chain of Thought (Visual CoT)</strong>: a pre-training framework that encourages a model to reason about how a scene evolves before predicting what it will look like. We instantiate this principle in <strong>FlowVLA</strong>, which predicts a future frame (v<sub>t+1</sub>) only after generating an intermediate optical flow representation (f<sub>t</sub>) that encodes motion dynamics. This "<strong>v<sub>t</sub> → f<sub>t</sub> → v<sub>t+1</sub></strong>" reasoning process is implemented within a single autoregressive Transformer, guiding the model to learn disentangled dynamics. As a result, FlowVLA produces coherent visual predictions and facilitates more efficient policy learning. Experiments on challenging robotics manipulation benchmarks demonstrate state-of-the-art performance with substantially improved sample efficiency, pointing toward a more principled foundation for world modeling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Approach -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified">
              <p>
                  FlowVLA implements the Visual CoT principle within a single, unified Transformer. We achieve this with a simple yet powerful design by encoding both RGB frames (appearance) and optical flow fields (motion) into the same token vocabulary using a shared VQ-GAN. The model then learns an interleaved sequence of <code>[frame, flow, frame, flow, ...]</code>, explicitly forcing it to predict motion before predicting the next state.
              </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <img src="static/images/flowvla_architecture.png" alt="FlowVLA Framework Architecture"/>
          <h2 class="subtitle has-text-centered">
            The FlowVLA Framework: (Left) Pre-training with Visual CoT. (Right) Fine-tuning for robotic control.
          </h2>
        </div>
      </div>
    </div>
</section>
<!-- End Approach -->

<!-- World Model Rollouts -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">World Model Rollouts: Seeing is Believing</h2>
          <div class="content has-text-justified">
              <p>
                  By reasoning about motion first, FlowVLA generates significantly sharper, more physically coherent, and longer-horizon video predictions compared to standard next-frame prediction models.
              </p>
          </div>
        </div>
      </div>
      <!-- LIBERO (Simulation) 的部分已被完全删除 -->

      <h3 class="title is-4 has-text-centered">Bridge V2 (Real-World)</h3>
      <!-- 只保留 Bridge V2 的图片 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
            <img src="static/images/bridge_rollout.png" alt="Bridge V2 rollout comparison"/>
        </div>
      </div>
      <!-- 更新了下方的说明文字，使其与图片内容保持一致 -->
      <h2 class="subtitle has-text-centered">
        On the real-world Bridge V2 task, the standard approach (UniVLA, middle row) produces blurry predictions. FlowVLA (bottom row) maintains sharp, physically plausible rollouts.
      </h2>
    </div>
</section>
<!-- End World Model Rollouts -->



<!-- Benchmark Performance -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Benchmark Performance</h2>
          <div class="content has-text-justified">
            <p>
                FlowVLA establishes a new state-of-the-art on two challenging robotics benchmarks, demonstrating the effectiveness of our approach.
            </p>
          </div>
        </div>
      </div>
      <h3 class="title is-4 has-text-centered">LIBERO Benchmark</h3>
      <!-- 添加了布局容器来控制图片宽度 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <img src="static/images/libero_table.png" alt="LIBERO Benchmark Results"/>
        </div>
      </div>
      <p class="has-text-centered is-size-6">FlowVLA outperforms prior methods across all task suites, with the largest gains in the <b>Long</b>-horizon setting.</p>
      <br>
      <h3 class="title is-4 has-text-centered">SimplerEnv Benchmark</h3>
      <!-- 为第二张图片也添加了布局容器 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <img src="static/images/simplerenv_table.png" alt="SimplerEnv Benchmark Results"/>
        </div>
      </div>
      <p class="has-text-centered is-size-6">FlowVLA shows superior robustness to significant visual domain shifts, nearly doubling performance on "Stack Block".</p>
    </div>
</section>
<!-- End Benchmark Performance -->




<!-- Sample Efficiency -->
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Superior Sample Efficiency</h2>
                <div class="content has-text-justified">
                    <p>
                        A key benefit of our Visual CoT is dramatically improved sample efficiency. FlowVLA learns faster and better, especially when data is scarce. In the low-data regime (right), it achieves a <strong>55% higher success rate</strong> than the baseline.
                    </p>
                </div>
            </div>
        </div>
        <div class="columns is-centered">
            <div class="column is-full-width">
                <img src="static/images/sample_efficiency_.png" alt="Training efficiency comparison"/>
            </div>
        </div>
    </div>
</section>
<!-- End Sample Efficiency -->

<!-- Ablation Studies -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Ablation Studies</h2>
                <div class="content has-text-justified">
                    <p>
                        Our design choices are critical for success. The results confirm that the full <strong>Visual CoT structure</strong>, direct <strong>supervision on flow</strong>, and the <strong>interleaved sequence</strong> format are all essential for FlowVLA's performance. Removing any key component leads to a significant drop in success rate.
                    </p>
                </div>
            </div>
        </div>
        <div class="columns is-centered">
            <!-- The class here is changed to 'is-half' to make the image smaller -->
            <div class="column is-half">
                <img src="static/images/ablation_table.png" alt="Ablation study results"/>
            </div>
        </div>
    </div>
</section>
<!-- End Ablation Studies -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhong2025flowvla,
  author    = {Zhong, Zhide and Yan, Haodong and Li, Junfeng and Liu, Xiangchen and Gong, Xin and Song, Wenxuan and Chen, Jiayi and Li, Haoang},
  title     = {FlowVLA: Thinking in Motion with a Visual Chain of Thought},
  journal   = {arXiv preprint arXiv:XXXX.XXXXX},
  year      = {2025}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
